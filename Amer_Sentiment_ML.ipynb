{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amer_Sentiment_ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIpA/Q0l0V5ewA9FAHr6z2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfsadi/amerSentiment/blob/main/Amer_Sentiment_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOhQKhsn1QSv"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTrni2Sh1a_X"
      },
      "source": [
        "!cp /content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin /content/drive/MyDrive/   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NsYNmUaAFM0"
      },
      "source": [
        "!pip install -q hazm\r\n",
        "!pip install -q clean-text[gpl]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3tj0IpwAi2w"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "import hazm\r\n",
        "from cleantext import clean\r\n",
        "\r\n",
        "import plotly.express as px\r\n",
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import json\r\n",
        "import copy\r\n",
        "import collections"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emOAvlMSBR4N"
      },
      "source": [
        "train = pd.read_csv('/content/train.csv', error_bad_lines=False, delimiter='\\t')\r\n",
        "dev = pd.read_csv('/content/dev.csv', error_bad_lines=False, delimiter='\\t')\r\n",
        "test = pd.read_csv('/content/test.csv', error_bad_lines=False, delimiter='\\t')\r\n",
        "\r\n",
        "train = train[['comment', 'label', 'label_id']]\r\n",
        "dev = dev[['comment', 'label', 'label_id']]\r\n",
        "test = test[['comment', 'label', 'label_id']]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f9c6K6SKz46"
      },
      "source": [
        "train['comment_len_by_words'] = train['comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "dev['comment_len_by_words'] = dev['comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "test['comment_len_by_words'] = test['comment'].apply(lambda t: len(hazm.word_tokenize(t)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX9FMrgrK1HF"
      },
      "source": [
        "min_max_len = test[\"comment_len_by_words\"].min(), test[\"comment_len_by_words\"].max()\r\n",
        "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIYqQKEZLCCQ"
      },
      "source": [
        "def data_gl_than(data, less_than=100.0, greater_than=0.0, col='comment_len_by_words'):\r\n",
        "    data_length = data[col].values\r\n",
        "    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\r\n",
        "    data_glt_rate = (data_glt / len(data_length)) * 100\r\n",
        "    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceN-ZXQ_LH1d"
      },
      "source": [
        "data_gl_than(dev, 40, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31_hU0dJLq7z"
      },
      "source": [
        "minlim, maxlim = 3, 40"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HI230gLtjA"
      },
      "source": [
        "# remove comments with the length of fewer than three words\r\n",
        "train['comment_len_by_words'] = train['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\r\n",
        "train = train.dropna(subset=['comment_len_by_words'])\r\n",
        "train = train.reset_index(drop=True)\r\n",
        "dev['comment_len_by_words'] = dev['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\r\n",
        "dev = dev.dropna(subset=['comment_len_by_words'])\r\n",
        "dev = dev.reset_index(drop=True)\r\n",
        "test['comment_len_by_words'] = test['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\r\n",
        "test = test.dropna(subset=['comment_len_by_words'])\r\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6AGNjw7L51E"
      },
      "source": [
        "fig = go.Figure()\r\n",
        "\r\n",
        "fig.add_trace(go.Histogram(\r\n",
        "    x=test['comment_len_by_words']\r\n",
        "))\r\n",
        "\r\n",
        "fig.update_layout(\r\n",
        "    title_text='Distribution of word counts within comments',\r\n",
        "    xaxis_title_text='Word Count',\r\n",
        "    yaxis_title_text='Frequency',\r\n",
        "    bargap=0.2,\r\n",
        "    bargroupgap=0.2)\r\n",
        "\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6mW6o4INSYr"
      },
      "source": [
        "def cleanhtml(raw_html):\r\n",
        "    cleanr = re.compile('<.*?>')\r\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\r\n",
        "    return cleantext\r\n",
        "\r\n",
        "def cleaning(text):\r\n",
        "    text = text.strip()\r\n",
        "    # regular cleaning\r\n",
        "    text = clean(text,\r\n",
        "        fix_unicode=True,\r\n",
        "        to_ascii=False,\r\n",
        "        lower=True,\r\n",
        "        no_line_breaks=True,\r\n",
        "        no_urls=True,\r\n",
        "        no_emails=True,\r\n",
        "        no_phone_numbers=True,\r\n",
        "        no_numbers=False,\r\n",
        "        no_digits=False,\r\n",
        "        no_currency_symbols=True,\r\n",
        "        no_punct=False,\r\n",
        "        replace_with_url=\"\",\r\n",
        "        replace_with_email=\"\",\r\n",
        "        replace_with_phone_number=\"\",\r\n",
        "        replace_with_number=\"\",\r\n",
        "        replace_with_digit=\"0\",\r\n",
        "        replace_with_currency_symbol=\"\",\r\n",
        "    )\r\n",
        "\r\n",
        "    # cleaning htmls\r\n",
        "    text = cleanhtml(text)\r\n",
        "    \r\n",
        "    # normalizing\r\n",
        "    normalizer = hazm.Normalizer()\r\n",
        "    text = normalizer.normalize(text)\r\n",
        "    \r\n",
        "    # removing wierd patterns\r\n",
        "    wierd_pattern = re.compile(\"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "        u\"\\U00002702-\\U000027B0\"\r\n",
        "        u\"\\U000024C2-\\U0001F251\"\r\n",
        "        u\"\\U0001f926-\\U0001f937\"\r\n",
        "        u'\\U00010000-\\U0010ffff'\r\n",
        "        u\"\\u200d\"\r\n",
        "        u\"\\u2640-\\u2642\"\r\n",
        "        u\"\\u2600-\\u2B55\"\r\n",
        "        u\"\\u23cf\"\r\n",
        "        u\"\\u23e9\"\r\n",
        "        u\"\\u231a\"\r\n",
        "        u\"\\u3030\"\r\n",
        "        u\"\\ufe0f\"\r\n",
        "        u\"\\u2069\"\r\n",
        "        u\"\\u2066\"\r\n",
        "        # u\"\\u200c\"\r\n",
        "        u\"\\u2068\"\r\n",
        "        u\"\\u2067\"\r\n",
        "        \"]+\", flags=re.UNICODE)\r\n",
        "    \r\n",
        "    text = wierd_pattern.sub(r'', text)\r\n",
        "    \r\n",
        "    # removing extra spaces, hashtags\r\n",
        "    text = re.sub(\"#\", \"\", text)\r\n",
        "    text = re.sub(\"\\s+\", \" \", text)\r\n",
        "    \r\n",
        "    return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HspDVCrGNVbc"
      },
      "source": [
        "# cleaning comments\r\n",
        "train['cleaned_comment'] = train['comment'].apply(cleaning)\r\n",
        "dev['cleaned_comment'] = dev['comment'].apply(cleaning)\r\n",
        "test['cleaned_comment'] = test['comment'].apply(cleaning)\r\n",
        "\r\n",
        "\r\n",
        "# calculate the length of comments based on their words\r\n",
        "train['cleaned_comment_len_by_words'] = train['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "dev['cleaned_comment_len_by_words'] = dev['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "test['cleaned_comment_len_by_words'] = test['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "\r\n",
        "# remove comments with the length of fewer than three words\r\n",
        "train['cleaned_comment_len_by_words'] = train['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\r\n",
        "train = train.dropna(subset=['cleaned_comment_len_by_words'])\r\n",
        "train = train.reset_index(drop=True)\r\n",
        "dev['cleaned_comment_len_by_words'] = dev['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\r\n",
        "dev = dev.dropna(subset=['cleaned_comment_len_by_words'])\r\n",
        "dev = dev.reset_index(drop=True)\r\n",
        "test['cleaned_comment_len_by_words'] = test['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\r\n",
        "test = test.dropna(subset=['cleaned_comment_len_by_words'])\r\n",
        "test = test.reset_index(drop=True)\r\n",
        "\r\n",
        "train.head()\r\n",
        "dev.head()\r\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWr7YbehFl8s"
      },
      "source": [
        "import pickle\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "X_train= train['comment']\r\n",
        "X_test= test['comment']\r\n",
        "y_train= train['label']\r\n",
        "y_test= test['label']\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\r\n",
        "X_train = vectorizer.fit_transform(X_train)\r\n",
        "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\r\n",
        "X_test = vectorizer.transform(X_test)\r\n",
        "with open('vectorizer.pk', 'wb') as fin:\r\n",
        "    pickle.dump(vectorizer, fin)\r\n",
        "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\r\n",
        "#text_classifier.fit(train['comment'], train['label_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8aELJqmUXo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5dd76e-a89b-4c22-a842-54495f2cf600"
      },
      "source": [
        "import logging\r\n",
        "import numpy as np\r\n",
        "from optparse import OptionParser\r\n",
        "import sys\r\n",
        "from time import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pickle\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\r\n",
        "from sklearn.feature_selection import SelectFromModel\r\n",
        "from sklearn.feature_selection import SelectKBest, chi2\r\n",
        "from sklearn.linear_model import RidgeClassifier\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.linear_model import Perceptron\r\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\r\n",
        "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.neighbors import NearestCentroid\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.utils.extmath import density\r\n",
        "from sklearn import metrics\r\n",
        "def benchmark(clf):\r\n",
        "    print('_' * 80)\r\n",
        "    print(\"Training: \")\r\n",
        "    print(clf)\r\n",
        "    t0 = time()\r\n",
        "    clf.fit(X_train, y_train)\r\n",
        "    train_time = time() - t0\r\n",
        "    print(\"train time: %0.3fs\" % train_time)\r\n",
        "\r\n",
        "    t0 = time()\r\n",
        "    pred = clf.predict(X_test)\r\n",
        "    test_time = time() - t0\r\n",
        "    print(\"test time:  %0.3fs\" % test_time)\r\n",
        "\r\n",
        "    score = metrics.accuracy_score(y_test, pred)\r\n",
        "    print(\"accuracy:   %0.3f\" % score)\r\n",
        "\r\n",
        "    if hasattr(clf, 'coef_'):\r\n",
        "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\r\n",
        "        print(\"density: %f\" % density(clf.coef_))\r\n",
        "    print(\"classification report:\")\r\n",
        "    print(metrics.classification_report(y_test, pred,\r\n",
        "                                            target_names=['HAPPY','SAD']))\r\n",
        "\r\n",
        "    print(\"confusion matrix:\")\r\n",
        "    print(metrics.confusion_matrix(y_test, pred))\r\n",
        "\r\n",
        "    print()\r\n",
        "    clf_descr = str(clf).split('(')[0]\r\n",
        "    pickle.dump(clf, open(name, 'wb'))\r\n",
        "    return clf_descr, score, train_time, test_time\r\n",
        "\r\n",
        "\r\n",
        "results = []\r\n",
        "for clf, name in (\r\n",
        "        (RidgeClassifier(tol=1e-2, solver=\"sag\"), \"Ridge Classifier\"),\r\n",
        "        (Perceptron(max_iter=50), \"Perceptron\"),\r\n",
        "        (PassiveAggressiveClassifier(max_iter=50),\r\n",
        "         \"Passive-Aggressive\"),\r\n",
        "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\r\n",
        "        (RandomForestClassifier(), \"Random forest\")):\r\n",
        "    print('=' * 80)\r\n",
        "    print(name)\r\n",
        "    results.append(benchmark(clf))\r\n",
        "\r\n",
        "for penalty in [\"l2\", \"l1\"]:\r\n",
        "    print('=' * 80)\r\n",
        "    print(\"%s penalty\" % penalty.upper())\r\n",
        "    # Train Liblinear model\r\n",
        "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\r\n",
        "                                       tol=1e-3)))\r\n",
        "\r\n",
        "    # Train SGD model\r\n",
        "    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\r\n",
        "                                           penalty=penalty)))\r\n",
        "\r\n",
        "\r\n",
        "# Train sparse Naive Bayes classifiers\r\n",
        "print('=' * 80)\r\n",
        "print(\"Naive Bayes\")\r\n",
        "results.append(benchmark(MultinomialNB(alpha=.01)))\r\n",
        "results.append(benchmark(BernoulliNB(alpha=.01)))\r\n",
        "results.append(benchmark(ComplementNB(alpha=.1)))\r\n",
        "\r\n",
        "print('=' * 80)\r\n",
        "print(\"LinearSVC with L1-based feature selection\")\r\n",
        "# The smaller C, the stronger the regularization.\r\n",
        "# The more regularization, the more sparsity.\r\n",
        "results.append(benchmark(Pipeline([\r\n",
        "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\r\n",
        "                                                  tol=1e-3))),\r\n",
        "  ('classification', LinearSVC(penalty=\"l2\"))])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Ridge Classifier\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
            "                max_iter=None, normalize=False, random_state=None, solver='sag',\n",
            "                tol=0.01)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:558: UserWarning:\n",
            "\n",
            "\"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train time: 0.651s\n",
            "test time:  0.001s\n",
            "accuracy:   0.838\n",
            "dimensionality: 21662\n",
            "density: 1.000000\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.87      0.82      0.84      3295\n",
            "         SAD       0.81      0.86      0.84      3037\n",
            "\n",
            "    accuracy                           0.84      6332\n",
            "   macro avg       0.84      0.84      0.84      6332\n",
            "weighted avg       0.84      0.84      0.84      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2687  608]\n",
            " [ 417 2620]]\n",
            "\n",
            "================================================================================\n",
            "Perceptron\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
            "           fit_intercept=True, max_iter=50, n_iter_no_change=5, n_jobs=None,\n",
            "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
            "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.223s\n",
            "test time:  0.001s\n",
            "accuracy:   0.775\n",
            "dimensionality: 21662\n",
            "density: 0.768489\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.75      0.86      0.80      3295\n",
            "         SAD       0.82      0.68      0.74      3037\n",
            "\n",
            "    accuracy                           0.77      6332\n",
            "   macro avg       0.78      0.77      0.77      6332\n",
            "weighted avg       0.78      0.77      0.77      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2827  468]\n",
            " [ 957 2080]]\n",
            "\n",
            "================================================================================\n",
            "Passive-Aggressive\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
            "                            early_stopping=False, fit_intercept=True,\n",
            "                            loss='hinge', max_iter=50, n_iter_no_change=5,\n",
            "                            n_jobs=None, random_state=None, shuffle=True,\n",
            "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
            "                            warm_start=False)\n",
            "train time: 0.466s\n",
            "test time:  0.001s\n",
            "accuracy:   0.810\n",
            "dimensionality: 21662\n",
            "density: 0.883575\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.82      0.82      0.82      3295\n",
            "         SAD       0.80      0.80      0.80      3037\n",
            "\n",
            "    accuracy                           0.81      6332\n",
            "   macro avg       0.81      0.81      0.81      6332\n",
            "weighted avg       0.81      0.81      0.81      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2706  589]\n",
            " [ 614 2423]]\n",
            "\n",
            "================================================================================\n",
            "kNN\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
            "                     weights='uniform')\n",
            "train time: 0.062s\n",
            "test time:  10.491s\n",
            "accuracy:   0.795\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.82      0.78      0.80      3295\n",
            "         SAD       0.77      0.81      0.79      3037\n",
            "\n",
            "    accuracy                           0.79      6332\n",
            "   macro avg       0.79      0.80      0.79      6332\n",
            "weighted avg       0.80      0.79      0.79      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2560  735]\n",
            " [ 566 2471]]\n",
            "\n",
            "================================================================================\n",
            "Random forest\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=None, max_features='auto',\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "train time: 115.667s\n",
            "test time:  0.498s\n",
            "accuracy:   0.848\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.89      0.81      0.85      3295\n",
            "         SAD       0.81      0.89      0.85      3037\n",
            "\n",
            "    accuracy                           0.85      6332\n",
            "   macro avg       0.85      0.85      0.85      6332\n",
            "weighted avg       0.85      0.85      0.85      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2660  635]\n",
            " [ 327 2710]]\n",
            "\n",
            "================================================================================\n",
            "L2 penalty\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
            "          verbose=0)\n",
            "train time: 0.767s\n",
            "test time:  0.001s\n",
            "accuracy:   0.836\n",
            "dimensionality: 21662\n",
            "density: 1.000000\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.86      0.82      0.84      3295\n",
            "         SAD       0.81      0.85      0.83      3037\n",
            "\n",
            "    accuracy                           0.84      6332\n",
            "   macro avg       0.84      0.84      0.84      6332\n",
            "weighted avg       0.84      0.84      0.84      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2702  593]\n",
            " [ 445 2592]]\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
            "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
            "              random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.195s\n",
            "test time:  0.001s\n",
            "accuracy:   0.852\n",
            "dimensionality: 21662\n",
            "density: 0.824578\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.90      0.80      0.85      3295\n",
            "         SAD       0.81      0.90      0.85      3037\n",
            "\n",
            "    accuracy                           0.85      6332\n",
            "   macro avg       0.86      0.85      0.85      6332\n",
            "weighted avg       0.86      0.85      0.85      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2652  643]\n",
            " [ 292 2745]]\n",
            "\n",
            "================================================================================\n",
            "L1 penalty\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
            "          verbose=0)\n",
            "train time: 0.580s\n",
            "test time:  0.001s\n",
            "accuracy:   0.839\n",
            "dimensionality: 21662\n",
            "density: 0.253347\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.86      0.82      0.84      3295\n",
            "         SAD       0.81      0.86      0.84      3037\n",
            "\n",
            "    accuracy                           0.84      6332\n",
            "   macro avg       0.84      0.84      0.84      6332\n",
            "weighted avg       0.84      0.84      0.84      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2701  594]\n",
            " [ 426 2611]]\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
            "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
            "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=50,\n",
            "              n_iter_no_change=5, n_jobs=None, penalty='l1', power_t=0.5,\n",
            "              random_state=None, shuffle=True, tol=0.001,\n",
            "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
            "train time: 0.199s\n",
            "test time:  0.001s\n",
            "accuracy:   0.844\n",
            "dimensionality: 21662\n",
            "density: 0.017958\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.90      0.79      0.84      3295\n",
            "         SAD       0.80      0.91      0.85      3037\n",
            "\n",
            "    accuracy                           0.84      6332\n",
            "   macro avg       0.85      0.85      0.84      6332\n",
            "weighted avg       0.85      0.84      0.84      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2591  704]\n",
            " [ 284 2753]]\n",
            "\n",
            "================================================================================\n",
            "Naive Bayes\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
            "train time: 0.174s\n",
            "test time:  0.001s\n",
            "accuracy:   0.811\n",
            "dimensionality: 21662\n",
            "density: 1.000000\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.85      0.77      0.81      3295\n",
            "         SAD       0.78      0.85      0.81      3037\n",
            "\n",
            "    accuracy                           0.81      6332\n",
            "   macro avg       0.81      0.81      0.81      6332\n",
            "weighted avg       0.81      0.81      0.81      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2552  743]\n",
            " [ 451 2586]]\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
            "train time: 0.177s\n",
            "test time:  0.009s\n",
            "accuracy:   0.814\n",
            "dimensionality: 21662\n",
            "density: 1.000000\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.83      0.81      0.82      3295\n",
            "         SAD       0.80      0.82      0.81      3037\n",
            "\n",
            "    accuracy                           0.81      6332\n",
            "   macro avg       0.81      0.81      0.81      6332\n",
            "weighted avg       0.81      0.81      0.81      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2668  627]\n",
            " [ 552 2485]]\n",
            "\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "ComplementNB(alpha=0.1, class_prior=None, fit_prior=True, norm=False)\n",
            "train time: 0.169s\n",
            "test time:  0.002s\n",
            "accuracy:   0.822\n",
            "dimensionality: 21662\n",
            "density: 1.000000\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.87      0.77      0.82      3295\n",
            "         SAD       0.78      0.88      0.83      3037\n",
            "\n",
            "    accuracy                           0.82      6332\n",
            "   macro avg       0.83      0.82      0.82      6332\n",
            "weighted avg       0.83      0.82      0.82      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2541  754]\n",
            " [ 374 2663]]\n",
            "\n",
            "================================================================================\n",
            "LinearSVC with L1-based feature selection\n",
            "________________________________________________________________________________\n",
            "Training: \n",
            "Pipeline(memory=None,\n",
            "         steps=[('feature_selection',\n",
            "                 SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None,\n",
            "                                                     dual=False,\n",
            "                                                     fit_intercept=True,\n",
            "                                                     intercept_scaling=1,\n",
            "                                                     loss='squared_hinge',\n",
            "                                                     max_iter=1000,\n",
            "                                                     multi_class='ovr',\n",
            "                                                     penalty='l1',\n",
            "                                                     random_state=None,\n",
            "                                                     tol=0.001, verbose=0),\n",
            "                                 max_features=None, norm_order=1, prefit=False,\n",
            "                                 threshold=None)),\n",
            "                ('classification',\n",
            "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
            "                           fit_intercept=True, intercept_scaling=1,\n",
            "                           loss='squared_hinge', max_iter=1000,\n",
            "                           multi_class='ovr', penalty='l2', random_state=None,\n",
            "                           tol=0.0001, verbose=0))],\n",
            "         verbose=False)\n",
            "train time: 1.518s\n",
            "test time:  0.002s\n",
            "accuracy:   0.838\n",
            "classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       HAPPY       0.86      0.82      0.84      3295\n",
            "         SAD       0.82      0.86      0.84      3037\n",
            "\n",
            "    accuracy                           0.84      6332\n",
            "   macro avg       0.84      0.84      0.84      6332\n",
            "weighted avg       0.84      0.84      0.84      6332\n",
            "\n",
            "confusion matrix:\n",
            "[[2705  590]\n",
            " [ 437 2600]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa8vOZo9Xv86",
        "outputId": "8bb86ca5-4919-4e7d-fac8-bdf5a5ccd135"
      },
      "source": [
        "model = pickle.load(open('kNN', 'rb'))\r\n",
        "sample=['شیرینی کیفیت خوبی نداشت']\r\n",
        "vt=pickle.load(open(\"vectorizer.pk\", \"rb\"))\r\n",
        "unseen_tfidf = vt.transform(sample)\r\n",
        "result= model.predict(unseen_tfidf)\r\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['SAD']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}