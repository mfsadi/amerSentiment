{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amer_Sentiment_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzVRonaMPHwpS9rC3Vh9MT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ccd4570fce9947d09ffec6ecac13afd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_31f0b07e81b74027a3e1d209d1963438",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88440d26c7614efa80e89fcf25b37f89",
              "IPY_MODEL_821ea8c92f0d4175aa89766671c1041a"
            ]
          }
        },
        "31f0b07e81b74027a3e1d209d1963438": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88440d26c7614efa80e89fcf25b37f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90c7a3d61d6a4e6eb450ea8dd17b0810",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2218f9adc0bd4aca84aa5ab604e15ee4"
          }
        },
        "821ea8c92f0d4175aa89766671c1041a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2defe0fcd53148cd9e320346c1444ec5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:00&lt;00:00,  1.52it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_552451552327408db950d2d20368351b"
          }
        },
        "90c7a3d61d6a4e6eb450ea8dd17b0810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2218f9adc0bd4aca84aa5ab604e15ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2defe0fcd53148cd9e320346c1444ec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "552451552327408db950d2d20368351b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfsadi/amerSentiment/blob/main/Amer_Sentiment_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOhQKhsn1QSv"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTrni2Sh1a_X"
      },
      "source": [
        "!cp /content/drive/MyDrive/pytorch_model.bin /content/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NsYNmUaAFM0"
      },
      "source": [
        "!pip install -q transformers\r\n",
        "!pip install -q hazm\r\n",
        "!pip install -q clean-text[gpl]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3tj0IpwAi2w"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "import hazm\r\n",
        "from cleantext import clean\r\n",
        "\r\n",
        "import plotly.express as px\r\n",
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "import json\r\n",
        "import copy\r\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emOAvlMSBR4N"
      },
      "source": [
        "train = pd.read_csv('/content/train.csv', error_bad_lines=False, delimiter='\\t')\r\n",
        "dev = pd.read_csv('/content/dev.csv', error_bad_lines=False, delimiter='\\t')\r\n",
        "test = pd.read_csv('/content/test.csv', error_bad_lines=False, delimiter='\\t')\r\n",
        "\r\n",
        "train = train[['comment', 'label', 'label_id']]\r\n",
        "dev = dev[['comment', 'label', 'label_id']]\r\n",
        "test = test[['comment', 'label', 'label_id']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f9c6K6SKz46"
      },
      "source": [
        "train['comment_len_by_words'] = train['comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "dev['comment_len_by_words'] = dev['comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "test['comment_len_by_words'] = test['comment'].apply(lambda t: len(hazm.word_tokenize(t)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX9FMrgrK1HF"
      },
      "source": [
        "min_max_len = test[\"comment_len_by_words\"].min(), test[\"comment_len_by_words\"].max()\r\n",
        "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIYqQKEZLCCQ"
      },
      "source": [
        "def data_gl_than(data, less_than=100.0, greater_than=0.0, col='comment_len_by_words'):\r\n",
        "    data_length = data[col].values\r\n",
        "    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\r\n",
        "    data_glt_rate = (data_glt / len(data_length)) * 100\r\n",
        "    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceN-ZXQ_LH1d"
      },
      "source": [
        "data_gl_than(dev, 40, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31_hU0dJLq7z"
      },
      "source": [
        "minlim, maxlim = 3, 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26HI230gLtjA"
      },
      "source": [
        "# remove comments with the length of fewer than three words\r\n",
        "train['comment_len_by_words'] = train['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\r\n",
        "train = train.dropna(subset=['comment_len_by_words'])\r\n",
        "train = train.reset_index(drop=True)\r\n",
        "dev['comment_len_by_words'] = dev['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\r\n",
        "dev = dev.dropna(subset=['comment_len_by_words'])\r\n",
        "dev = dev.reset_index(drop=True)\r\n",
        "test['comment_len_by_words'] = test['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\r\n",
        "test = test.dropna(subset=['comment_len_by_words'])\r\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6AGNjw7L51E"
      },
      "source": [
        "fig = go.Figure()\r\n",
        "\r\n",
        "fig.add_trace(go.Histogram(\r\n",
        "    x=test['comment_len_by_words']\r\n",
        "))\r\n",
        "\r\n",
        "fig.update_layout(\r\n",
        "    title_text='Distribution of word counts within comments',\r\n",
        "    xaxis_title_text='Word Count',\r\n",
        "    yaxis_title_text='Frequency',\r\n",
        "    bargap=0.2,\r\n",
        "    bargroupgap=0.2)\r\n",
        "\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6mW6o4INSYr"
      },
      "source": [
        "def cleanhtml(raw_html):\r\n",
        "    cleanr = re.compile('<.*?>')\r\n",
        "    cleantext = re.sub(cleanr, '', raw_html)\r\n",
        "    return cleantext\r\n",
        "\r\n",
        "def cleaning(text):\r\n",
        "    text = text.strip()\r\n",
        "    # regular cleaning\r\n",
        "    text = clean(text,\r\n",
        "        fix_unicode=True,\r\n",
        "        to_ascii=False,\r\n",
        "        lower=True,\r\n",
        "        no_line_breaks=True,\r\n",
        "        no_urls=True,\r\n",
        "        no_emails=True,\r\n",
        "        no_phone_numbers=True,\r\n",
        "        no_numbers=False,\r\n",
        "        no_digits=False,\r\n",
        "        no_currency_symbols=True,\r\n",
        "        no_punct=False,\r\n",
        "        replace_with_url=\"\",\r\n",
        "        replace_with_email=\"\",\r\n",
        "        replace_with_phone_number=\"\",\r\n",
        "        replace_with_number=\"\",\r\n",
        "        replace_with_digit=\"0\",\r\n",
        "        replace_with_currency_symbol=\"\",\r\n",
        "    )\r\n",
        "\r\n",
        "    # cleaning htmls\r\n",
        "    text = cleanhtml(text)\r\n",
        "    \r\n",
        "    # normalizing\r\n",
        "    normalizer = hazm.Normalizer()\r\n",
        "    text = normalizer.normalize(text)\r\n",
        "    \r\n",
        "    # removing wierd patterns\r\n",
        "    wierd_pattern = re.compile(\"[\"\r\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
        "        u\"\\U00002702-\\U000027B0\"\r\n",
        "        u\"\\U000024C2-\\U0001F251\"\r\n",
        "        u\"\\U0001f926-\\U0001f937\"\r\n",
        "        u'\\U00010000-\\U0010ffff'\r\n",
        "        u\"\\u200d\"\r\n",
        "        u\"\\u2640-\\u2642\"\r\n",
        "        u\"\\u2600-\\u2B55\"\r\n",
        "        u\"\\u23cf\"\r\n",
        "        u\"\\u23e9\"\r\n",
        "        u\"\\u231a\"\r\n",
        "        u\"\\u3030\"\r\n",
        "        u\"\\ufe0f\"\r\n",
        "        u\"\\u2069\"\r\n",
        "        u\"\\u2066\"\r\n",
        "        # u\"\\u200c\"\r\n",
        "        u\"\\u2068\"\r\n",
        "        u\"\\u2067\"\r\n",
        "        \"]+\", flags=re.UNICODE)\r\n",
        "    \r\n",
        "    text = wierd_pattern.sub(r'', text)\r\n",
        "    \r\n",
        "    # removing extra spaces, hashtags\r\n",
        "    text = re.sub(\"#\", \"\", text)\r\n",
        "    text = re.sub(\"\\s+\", \" \", text)\r\n",
        "    \r\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HspDVCrGNVbc"
      },
      "source": [
        "# cleaning comments\r\n",
        "train['cleaned_comment'] = train['comment'].apply(cleaning)\r\n",
        "dev['cleaned_comment'] = dev['comment'].apply(cleaning)\r\n",
        "test['cleaned_comment'] = test['comment'].apply(cleaning)\r\n",
        "\r\n",
        "\r\n",
        "# calculate the length of comments based on their words\r\n",
        "train['cleaned_comment_len_by_words'] = train['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "dev['cleaned_comment_len_by_words'] = dev['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "test['cleaned_comment_len_by_words'] = test['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\r\n",
        "\r\n",
        "# remove comments with the length of fewer than three words\r\n",
        "train['cleaned_comment_len_by_words'] = train['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\r\n",
        "train = train.dropna(subset=['cleaned_comment_len_by_words'])\r\n",
        "train = train.reset_index(drop=True)\r\n",
        "dev['cleaned_comment_len_by_words'] = dev['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\r\n",
        "dev = dev.dropna(subset=['cleaned_comment_len_by_words'])\r\n",
        "dev = dev.reset_index(drop=True)\r\n",
        "test['cleaned_comment_len_by_words'] = test['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\r\n",
        "test = test.dropna(subset=['cleaned_comment_len_by_words'])\r\n",
        "test = test.reset_index(drop=True)\r\n",
        "\r\n",
        "train.head()\r\n",
        "dev.head()\r\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm5FAF8kNuPz"
      },
      "source": [
        "fig = go.Figure()\r\n",
        "\r\n",
        "groupby_label = test.groupby('label_id')['label_id'].count()\r\n",
        "\r\n",
        "fig.add_trace(go.Bar(\r\n",
        "    x=list(sorted(groupby_label.index)),\r\n",
        "    y=groupby_label.tolist(),\r\n",
        "    text=groupby_label.tolist(),\r\n",
        "    textposition='auto'\r\n",
        "))\r\n",
        "\r\n",
        "fig.update_layout(\r\n",
        "    title_text='Distribution of label within comments [DATA]',\r\n",
        "    xaxis_title_text='Label',\r\n",
        "    yaxis_title_text='Frequency',\r\n",
        "    bargap=0.2,\r\n",
        "    bargroupgap=0.2)\r\n",
        "\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zLV38Q_R3k_"
      },
      "source": [
        "**PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BTBe6RkRZJ5"
      },
      "source": [
        "from transformers import BertConfig, BertTokenizer\r\n",
        "from transformers import BertModel\r\n",
        "\r\n",
        "from transformers import AdamW\r\n",
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0xPQEMeSRIH"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(f'device: {device}')\r\n",
        "\r\n",
        "train_on_gpu = torch.cuda.is_available()\r\n",
        "\r\n",
        "if not train_on_gpu:\r\n",
        "    print('CUDA is not available.  Training on CPU ...')\r\n",
        "else:\r\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQqCxw2vSm4v"
      },
      "source": [
        "# general config\r\n",
        "MAX_LEN = 128\r\n",
        "TRAIN_BATCH_SIZE = 16\r\n",
        "VALID_BATCH_SIZE = 16\r\n",
        "TEST_BATCH_SIZE = 16\r\n",
        "\r\n",
        "EPOCHS = 3\r\n",
        "EEVERY_EPOCH = 1000\r\n",
        "LEARNING_RATE = 2e-5\r\n",
        "CLIP = 0.0\r\n",
        "\r\n",
        "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\r\n",
        "OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'\r\n",
        "\r\n",
        "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oUZtMRIZfk8",
        "outputId": "da04e251-6a5d-4fc8-9c95-8e21c153cdf4"
      },
      "source": [
        "labels = list(sorted(train['label'].unique()))\r\n",
        "label2id = {label: i for i, label in enumerate(labels)}\r\n",
        "id2label = {v: k for k, v in label2id.items()}\r\n",
        "\r\n",
        "print(f'label2id: {label2id}')\r\n",
        "print(f'id2label: {id2label}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label2id: {'HAPPY': 0, 'SAD': 1}\n",
            "id2label: {0: 'HAPPY', 1: 'SAD'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt15liZlSofS"
      },
      "source": [
        "# setup the tokenizer and configuration\r\n",
        "\r\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\r\n",
        "config = BertConfig.from_pretrained(\r\n",
        "    MODEL_NAME_OR_PATH, **{\r\n",
        "        'label2id': label2id,\r\n",
        "        'id2label': id2label,\r\n",
        "    })\r\n",
        "\r\n",
        "print(config.to_json_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jri1kWUeS9yX"
      },
      "source": [
        "idx = np.random.randint(0, len(train))\r\n",
        "sample_comment = train.iloc[idx]['comment']\r\n",
        "sample_label = train.iloc[idx]['label_id']\r\n",
        "\r\n",
        "print(f'Sample: \\n{sample_comment}\\n{sample_label}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoqIDeJQURgf"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_comment)\r\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
        "\r\n",
        "print(f'  Comment: {sample_comment}')\r\n",
        "print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\r\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU9yJsVpUbE0"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\r\n",
        "    sample_comment,\r\n",
        "    max_length=32,\r\n",
        "    truncation=True,\r\n",
        "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\r\n",
        "    return_token_type_ids=True,\r\n",
        "    return_attention_mask=True,\r\n",
        "    padding='max_length',\r\n",
        "    return_tensors='pt',  # Return PyTorch tensors\r\n",
        ")\r\n",
        "\r\n",
        "print(f'Keys: {encoding.keys()}\\n')\r\n",
        "for k in encoding.keys():\r\n",
        "    print(f'{k}:\\n{encoding[k]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvWfGL7vUvG5"
      },
      "source": [
        "class SnapfoodDataset(torch.utils.data.Dataset):\r\n",
        "    \"\"\" Create a PyTorch dataset for Snapfood. \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, tokenizer, comments, targets=None, label_list=None, max_len=128):\r\n",
        "        self.comments = comments\r\n",
        "        self.targets = targets\r\n",
        "        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\r\n",
        "\r\n",
        "        self.tokenizer = tokenizer\r\n",
        "        self.max_len = max_len\r\n",
        "\r\n",
        "        \r\n",
        "        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\r\n",
        "    \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.comments)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        comment = str(self.comments[item])\r\n",
        "\r\n",
        "        if self.has_target:\r\n",
        "            target = self.label_map.get(str(self.targets[item]), str(self.targets[item]))\r\n",
        "\r\n",
        "        encoding = self.tokenizer.encode_plus(\r\n",
        "            comment,\r\n",
        "            add_special_tokens=True,\r\n",
        "            truncation=True,\r\n",
        "            max_length=self.max_len,\r\n",
        "            return_token_type_ids=True,\r\n",
        "            padding='max_length',\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt')\r\n",
        "        \r\n",
        "        inputs = {\r\n",
        "            'comment': comment,\r\n",
        "            'input_ids': encoding['input_ids'].flatten(),\r\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\r\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\r\n",
        "        }\r\n",
        "\r\n",
        "        if self.has_target:\r\n",
        "            inputs['targets'] = torch.tensor(target, dtype=torch.long)\r\n",
        "        \r\n",
        "        return inputs\r\n",
        "\r\n",
        "\r\n",
        "def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\r\n",
        "    dataset = SnapfoodDataset(\r\n",
        "        comments=x,\r\n",
        "        targets=y,\r\n",
        "        tokenizer=tokenizer,\r\n",
        "        max_len=max_len, \r\n",
        "        label_list=label_list)\r\n",
        "    \r\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_4JsBmfU_43"
      },
      "source": [
        "label_list = ['HAPPY', 'SAD']\r\n",
        "train_data_loader = create_data_loader(train['comment'].to_numpy(), train['label'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\r\n",
        "dev_data_loader = create_data_loader(dev['comment'].to_numpy(), dev['label'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\r\n",
        "test_data_loader = create_data_loader(test['comment'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAvXbVPBVaS3"
      },
      "source": [
        "sample_data = next(iter(train_data_loader))\r\n",
        "\r\n",
        "print(sample_data.keys())\r\n",
        "\r\n",
        "print(sample_data['comment'])\r\n",
        "print(sample_data['input_ids'].shape)\r\n",
        "print(sample_data['input_ids'][0, :])\r\n",
        "print(sample_data['attention_mask'].shape)\r\n",
        "print(sample_data['attention_mask'][0, :])\r\n",
        "print(sample_data['token_type_ids'].shape)\r\n",
        "print(sample_data['token_type_ids'][0, :])\r\n",
        "print(sample_data['targets'].shape)\r\n",
        "print(sample_data['targets'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeSOx_jdav6y"
      },
      "source": [
        "sample_test = next(iter(test_data_loader))\r\n",
        "print(sample_test.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orgJEsVZazRn"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPTzeX5saxx0"
      },
      "source": [
        "class SentimentModel(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, config):\r\n",
        "        super(SentimentModel, self).__init__()\r\n",
        "\r\n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH, return_dict=False)\r\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n",
        "    \r\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\r\n",
        "        _, pooled_output = self.bert(\r\n",
        "            input_ids=input_ids, \r\n",
        "            attention_mask=attention_mask, \r\n",
        "            token_type_ids=token_type_ids)\r\n",
        "        \r\n",
        "        pooled_output = self.dropout(pooled_output)\r\n",
        "        logits = self.classifier(pooled_output)\r\n",
        "        return logits "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WNxkB0ja_WB"
      },
      "source": [
        "import torch, gc\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "torch.cuda.empty_cache()\r\n",
        "pt_model = None\r\n",
        "\r\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxtirCXUbIPw"
      },
      "source": [
        "pt_model = SentimentModel(config=config)\r\n",
        "pt_model = pt_model.to(device)\r\n",
        "\r\n",
        "print('pt_model', type(pt_model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqPuLuSEbP49"
      },
      "source": [
        "# sample data output\r\n",
        "\r\n",
        "sample_data_comment = sample_data['comment']\r\n",
        "sample_data_input_ids = sample_data['input_ids']\r\n",
        "sample_data_attention_mask = sample_data['attention_mask']\r\n",
        "sample_data_token_type_ids = sample_data['token_type_ids']\r\n",
        "sample_data_targets = sample_data['targets']\r\n",
        "\r\n",
        "# available for using in GPU\r\n",
        "sample_data_input_ids = sample_data_input_ids.to(device)\r\n",
        "sample_data_attention_mask = sample_data_attention_mask.to(device)\r\n",
        "sample_data_token_type_ids = sample_data_token_type_ids.to(device)\r\n",
        "sample_data_targets = sample_data_targets.to(device)\r\n",
        "\r\n",
        "\r\n",
        "# outputs = F.softmax(\r\n",
        "#     pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids), \r\n",
        "#     dim=1)\r\n",
        "\r\n",
        "outputs = pt_model(sample_data_input_ids, sample_data_attention_mask, sample_data_token_type_ids)\r\n",
        "_, preds = torch.max(outputs, dim=1)\r\n",
        "\r\n",
        "print(outputs[:5, :])\r\n",
        "print(preds[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAeqqOz8ji0m"
      },
      "source": [
        "def simple_accuracy(y_true, y_pred):\r\n",
        "    return (y_true == y_pred).mean()\r\n",
        "\r\n",
        "def acc_and_f1(y_true, y_pred, average='weighted'):\r\n",
        "    acc = simple_accuracy(y_true, y_pred)\r\n",
        "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\r\n",
        "    return {\r\n",
        "        \"acc\": acc,\r\n",
        "        \"f1\": f1,\r\n",
        "    }\r\n",
        "\r\n",
        "def y_loss(y_true, y_pred, losses):\r\n",
        "    y_true = torch.stack(y_true).cpu().detach().numpy()\r\n",
        "    y_pred = torch.stack(y_pred).cpu().detach().numpy()\r\n",
        "    y = [y_true, y_pred]\r\n",
        "    loss = np.mean(losses)\r\n",
        "\r\n",
        "    return y, loss\r\n",
        "\r\n",
        "\r\n",
        "def eval_op(model, data_loader, loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    losses = []\r\n",
        "    y_pred = []\r\n",
        "    y_true = []\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\r\n",
        "            \r\n",
        "            input_ids = dl['input_ids']\r\n",
        "            attention_mask = dl['attention_mask']\r\n",
        "            token_type_ids = dl['token_type_ids']\r\n",
        "            targets = dl['targets']\r\n",
        "\r\n",
        "            # move tensors to GPU if CUDA is available\r\n",
        "            input_ids = input_ids.to(device)\r\n",
        "            attention_mask = attention_mask.to(device)\r\n",
        "            token_type_ids = token_type_ids.to(device)\r\n",
        "            targets = targets.to(device)\r\n",
        "\r\n",
        "            # compute predicted outputs by passing inputs to the model\r\n",
        "            outputs = model(\r\n",
        "                input_ids=input_ids,\r\n",
        "                attention_mask=attention_mask,\r\n",
        "                token_type_ids=token_type_ids)\r\n",
        "            \r\n",
        "            # convert output probabilities to predicted class\r\n",
        "            _, preds = torch.max(outputs, dim=1)\r\n",
        "\r\n",
        "            # calculate the batch loss\r\n",
        "            loss = loss_fn(outputs, targets)\r\n",
        "\r\n",
        "            # accumulate all the losses\r\n",
        "            losses.append(loss.item())\r\n",
        "\r\n",
        "            y_pred.extend(preds)\r\n",
        "            y_true.extend(targets)\r\n",
        "    \r\n",
        "    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\r\n",
        "    return eval_y, eval_loss\r\n",
        "\r\n",
        "\r\n",
        "def train_op(model, \r\n",
        "             data_loader, \r\n",
        "             loss_fn, \r\n",
        "             optimizer, \r\n",
        "             scheduler, \r\n",
        "             step=0, \r\n",
        "             print_every_step=100, \r\n",
        "             eval=False,\r\n",
        "             eval_cb=None,\r\n",
        "             eval_loss_min=np.Inf,\r\n",
        "             eval_data_loader=None, \r\n",
        "             clip=0.0):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "\r\n",
        "    losses = []\r\n",
        "    y_pred = []\r\n",
        "    y_true = []\r\n",
        "\r\n",
        "    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\r\n",
        "        step += 1\r\n",
        "\r\n",
        "        input_ids = dl['input_ids']\r\n",
        "        attention_mask = dl['attention_mask']\r\n",
        "        token_type_ids = dl['token_type_ids']\r\n",
        "        targets = dl['targets']\r\n",
        "\r\n",
        "        # move tensors to GPU if CUDA is available\r\n",
        "        input_ids = input_ids.to(device)\r\n",
        "        attention_mask = attention_mask.to(device)\r\n",
        "        token_type_ids = token_type_ids.to(device)\r\n",
        "        targets = targets.to(device)\r\n",
        "\r\n",
        "        # clear the gradients of all optimized variables\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        # compute predicted outputs by passing inputs to the model\r\n",
        "        outputs = model(\r\n",
        "            input_ids=input_ids,\r\n",
        "            attention_mask=attention_mask,\r\n",
        "            token_type_ids=token_type_ids)\r\n",
        "        \r\n",
        "        # convert output probabilities to predicted class\r\n",
        "        _, preds = torch.max(outputs, dim=1)\r\n",
        "\r\n",
        "        # calculate the batch loss\r\n",
        "        loss = loss_fn(outputs, targets)\r\n",
        "\r\n",
        "        # accumulate all the losses\r\n",
        "        losses.append(loss.item())\r\n",
        "\r\n",
        "        # compute gradient of the loss with respect to model parameters\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\r\n",
        "        if clip > 0.0:\r\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\r\n",
        "\r\n",
        "        # perform optimization step\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # perform scheduler step\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "        y_pred.extend(preds)\r\n",
        "        y_true.extend(targets)\r\n",
        "\r\n",
        "        if eval:\r\n",
        "            train_y, train_loss = y_loss(y_true, y_pred, losses)\r\n",
        "            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\r\n",
        "\r\n",
        "            if step % print_every_step == 0:\r\n",
        "                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\r\n",
        "                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\r\n",
        "\r\n",
        "                if hasattr(eval_cb, '__call__'):\r\n",
        "                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\r\n",
        "\r\n",
        "    train_y, train_loss = y_loss(y_true, y_pred, losses)\r\n",
        "\r\n",
        "    return train_y, train_loss, step, eval_loss_min"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6S-1MCcjlDK"
      },
      "source": [
        "optimizer = AdamW(pt_model.parameters(), lr=LEARNING_RATE, correct_bias=False)\r\n",
        "total_steps = len(train_data_loader) * EPOCHS\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer,\r\n",
        "    num_warmup_steps=0,\r\n",
        "    num_training_steps=total_steps\r\n",
        ")\r\n",
        "\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "step = 0\r\n",
        "eval_loss_min = np.Inf\r\n",
        "history = collections.defaultdict(list)\r\n",
        "\r\n",
        "\r\n",
        "def eval_callback(epoch, epochs, output_path):\r\n",
        "    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):\r\n",
        "        statement = ''\r\n",
        "        statement += 'Epoch: {}/{}...'.format(epoch, epochs)\r\n",
        "        statement += 'Step: {}...'.format(step)\r\n",
        "        \r\n",
        "        statement += 'Train Loss: {:.6f}...'.format(train_loss)\r\n",
        "        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])\r\n",
        "\r\n",
        "        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)\r\n",
        "        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])\r\n",
        "\r\n",
        "        print(statement)\r\n",
        "\r\n",
        "        if eval_loss <= eval_loss_min:\r\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\r\n",
        "                eval_loss_min,\r\n",
        "                eval_loss))\r\n",
        "            \r\n",
        "            torch.save(model.state_dict(), output_path)\r\n",
        "            eval_loss_min = eval_loss\r\n",
        "        \r\n",
        "        return eval_loss_min\r\n",
        "\r\n",
        "\r\n",
        "    return eval_cb\r\n",
        "\r\n",
        "\r\n",
        "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs... \"):\r\n",
        "    train_y, train_loss, step, eval_loss_min = train_op(\r\n",
        "        model=pt_model, \r\n",
        "        data_loader=train_data_loader, \r\n",
        "        loss_fn=loss_fn, \r\n",
        "        optimizer=optimizer, \r\n",
        "        scheduler=scheduler, \r\n",
        "        step=step, \r\n",
        "        print_every_step=EEVERY_EPOCH, \r\n",
        "        eval=True,\r\n",
        "        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_PATH),\r\n",
        "        eval_loss_min=eval_loss_min,\r\n",
        "        eval_data_loader=dev_data_loader, \r\n",
        "        clip=CLIP)\r\n",
        "    \r\n",
        "    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\r\n",
        "    \r\n",
        "    eval_y, eval_loss = eval_op(\r\n",
        "        model=pt_model, \r\n",
        "        data_loader=dev_data_loader, \r\n",
        "        loss_fn=loss_fn)\r\n",
        "    \r\n",
        "    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\r\n",
        "    \r\n",
        "    history['train_acc'].append(train_score['acc'])\r\n",
        "    history['train_loss'].append(train_loss)\r\n",
        "    history['val_acc'].append(eval_score['acc'])\r\n",
        "    history['val_loss'].append(eval_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZ6X_SD2DVy"
      },
      "source": [
        "**Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMq1Q9-v2F5h"
      },
      "source": [
        "def predict(model, comments, tokenizer, max_len=128, batch_size=32):\r\n",
        "    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None)\r\n",
        "    \r\n",
        "    predictions = []\r\n",
        "    prediction_probs = []\r\n",
        "\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        for dl in tqdm(data_loader, position=0):\r\n",
        "            input_ids = dl['input_ids']\r\n",
        "            attention_mask = dl['attention_mask']\r\n",
        "            token_type_ids = dl['token_type_ids']\r\n",
        "\r\n",
        "            # move tensors to GPU if CUDA is available\r\n",
        "            input_ids = input_ids.to(device)\r\n",
        "            attention_mask = attention_mask.to(device)\r\n",
        "            token_type_ids = token_type_ids.to(device)\r\n",
        "            \r\n",
        "            # compute predicted outputs by passing inputs to the model\r\n",
        "            outputs = model(\r\n",
        "                input_ids=input_ids,\r\n",
        "                attention_mask=attention_mask,\r\n",
        "                token_type_ids=token_type_ids)\r\n",
        "            \r\n",
        "            # convert output probabilities to predicted class\r\n",
        "            _, preds = torch.max(outputs, dim=1)\r\n",
        "\r\n",
        "            predictions.extend(preds)\r\n",
        "            prediction_probs.extend(F.softmax(outputs, dim=1))\r\n",
        "\r\n",
        "    predictions = torch.stack(predictions).cpu().detach().numpy()\r\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\r\n",
        "\r\n",
        "    return predictions, prediction_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gayJTIFH2UBv"
      },
      "source": [
        "test_comments = test['comment'].to_numpy()\r\n",
        "preds, probs = predict(pt_model, test_comments, tokenizer, max_len=128)\r\n",
        "\r\n",
        "print(preds.shape, probs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPbXWDr2Yzl"
      },
      "source": [
        "y_test, y_pred = [label_list.index(label) for label in test['label'].values], preds\r\n",
        "\r\n",
        "print(f'F1: {f1_score(y_test, y_pred, average=\"weighted\")}')\r\n",
        "print()\r\n",
        "print(classification_report(y_test, y_pred, target_names=label_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRFZjMmxGdqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f74897-ff0f-4506-f2c9-3c33557a9875"
      },
      "source": [
        "ptmodel1 = SentimentModel(config=config)\r\n",
        "checkpoint = torch.load('/content/pytorch_model.bin', map_location='cpu')\r\n",
        "try:\r\n",
        "    checkpoint.eval()\r\n",
        "except AttributeError as error:\r\n",
        "    print (error)\r\n",
        "### 'dict' object has no attribute 'eval'\r\n",
        "ptmodel1.load_state_dict(checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'collections.OrderedDict' object has no attribute 'eval'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv1Ni9wtKdtn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "ccd4570fce9947d09ffec6ecac13afd7",
            "31f0b07e81b74027a3e1d209d1963438",
            "88440d26c7614efa80e89fcf25b37f89",
            "821ea8c92f0d4175aa89766671c1041a",
            "90c7a3d61d6a4e6eb450ea8dd17b0810",
            "2218f9adc0bd4aca84aa5ab604e15ee4",
            "2defe0fcd53148cd9e320346c1444ec5",
            "552451552327408db950d2d20368351b"
          ]
        },
        "outputId": "cbcc6c60-631f-46e4-f247-287a86d0b782"
      },
      "source": [
        "test_comments = test['comment'].to_numpy()\r\n",
        "new_sample=[\"ØºØ°Ø§Ø´ÙˆÙ† ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯\"]\r\n",
        "new_sample=np.array(list(new_sample))\r\n",
        "preds, probs = predict(ptmodel1, new_sample, tokenizer, max_len=128)\r\n",
        "print(id2label[preds.item()], probs)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ccd4570fce9947d09ffec6ecac13afd7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "HAPPY [[0.99209994 0.00790007]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}